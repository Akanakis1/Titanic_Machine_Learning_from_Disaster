# ðŸš¢ Titanic Survival Prediction (Kaggle) â€” End-to-End ML Pipeline

**Dataset:** [Titanic - Machine Learning from Disaster â€“ Kaggle](https://www.kaggle.com/c/titanic)

Predicting passenger survival on the Titanic using a complete machine-learning workflow:  
**data cleaning â†’ feature engineering â†’ model comparison â†’ best-model selection â†’ Kaggle submission**.

**Best validation performance:** **XGBoost = 0.8444 accuracy**

---

## âœ¨ Highlights
- Built a reproducible pipeline that merges train/test, engineers' features, compares models, and exports a Kaggle-ready submission.
- Feature engineering from raw fields (e.g., **Title from Name**, **Deck/Floor from Cabin**, **FamilySize**, **Single**, **Embarked dummies**).
- Benchmarked **6 classification models** and automatically selected the best one by validation accuracy.
- Final output file: `data/final/Titanic_Machine_Learning_from_Disaster.csv`.

---

## ðŸ§  Approach (What I did)

### 1) Data preprocessing
- Merged train + test with a flag (`is_train`) to ensure consistent transformations.
- Encoded variables (e.g., `Sex` mapped to numeric).
- Imputed missing values:
  - `Age` â†’ median
  - `Fare` â†’ median
  - `Embarked` â†’ mode

### 2) Feature engineering (adds signal beyond baseline)
- **Title extraction** from passenger names + grouping rare titles.
- **Cabin deck (â€œFloorâ€)** extracted from Cabin and one-hot encoded (Aâ€“G).
- **Family features**
  - `FamilySize = SibSp + Parch + 1`
  - `Single` indicator
- One-hot encoding for Embarked (kept C and Q).

### 3) Modeling + evaluation
Models compared (same validation split, `random_state=42`):
- Logistic Regression (scaled features)
- SVC (scaled features)
- Random Forest
- Gradient Boosting
- AdaBoost
- XGBoost

Validation accuracy summary (best model highlighted):
| Model | Accuracy (Valid) |
|---|---:|
| Logistic Regression | 0.8333 |
| SVC | 0.8333 |
| Random Forest | 0.8222 |
| Gradient Boosting | 0.8333 |
| AdaBoost | 0.8000 |
| **XGBoost** | **0.8444** |

---

## ðŸ“ Repo structure
- `Titanic_Machine_Learning.py` â€” main pipeline (preprocessing â†’ modeling â†’ submission)
- `Titanic.ipynb` â€” notebook version (optional)
- `Exploratory_Data_Analysis_(EDA).ipynb` â€” EDA & visuals (optional)
- `data/train.csv`, `data/test.csv` â€” Kaggle dataset files (expected locally)
- `data/final/` â€” output submission file

---

## ðŸš€ How to run
> Note: the script expects dataset files under `data/` (see code paths).

### 1) Install dependencies
If you have a `requirements.txt`:
```bash
pip install -r requirements.txt
